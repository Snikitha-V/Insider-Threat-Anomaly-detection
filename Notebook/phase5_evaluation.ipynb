{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3003a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# --- ADJUST THIS depending on where you run the notebook ---\n",
    "# Option A: notebook is located inside the folder that contains user_features.csv, answers_r42.csv:\n",
    "BASE_DIR = \".\"   # run if you open the notebook from the folder containing the files\n",
    "\n",
    "# Option B: notebook is in project root and files are in Notebook/\n",
    "# BASE_DIR = \"Notebook\"\n",
    "\n",
    "# set to whichever works for your environment. Use prints below to confirm.\n",
    "print(\"Working base dir:\", os.path.abspath(BASE_DIR))\n",
    "print(\"Files in base dir:\", os.listdir(BASE_DIR))\n",
    "\n",
    "# expected filenames\n",
    "GT_FILE = os.path.join(BASE_DIR, \"answers_r42.csv\")                # ground truth\n",
    "PRED_FILE = os.path.join(BASE_DIR, \"results/anomaly_scores_by_user.csv\")  # model outputs (scores +/or label)\n",
    "FEATURES_FILE = os.path.join(BASE_DIR, \"user_features.csv\")        # optional: features used\n",
    "\n",
    "# ensure results folder exists\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results_phase5\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# quick existence checks\n",
    "for f in [GT_FILE, PRED_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        print(\"WARNING: file not found:\", f)\n",
    "    else:\n",
    "        print(\"Found:\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files (small prints for safety)\n",
    "gt = pd.read_csv(GT_FILE)\n",
    "pred = pd.read_csv(PRED_FILE)\n",
    "\n",
    "print(\"GROUND TRUTH sample:\")\n",
    "display(gt.head())\n",
    "print(\"GROUND TRUTH columns:\", gt.columns.tolist())\n",
    "\n",
    "print(\"\\nPREDICTIONS sample:\")\n",
    "display(pred.head())\n",
    "print(\"PREDICTIONS columns:\", pred.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find common user column name (try several possibilities)\n",
    "def find_user_col(df):\n",
    "    for c in df.columns:\n",
    "        if c.lower() in (\"user\",\"user_id\",\"employee\",\"employee_id\",\"id\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "u_gt = find_user_col(gt)\n",
    "u_pred = find_user_col(pred)\n",
    "print(\"gt user col:\", u_gt, \"pred user col:\", u_pred)\n",
    "if u_gt is None or u_pred is None:\n",
    "    raise KeyError(\"Could not find user key columns automatically. Rename your user id columns to 'user' or 'user_id'.\")\n",
    "\n",
    "# normalize\n",
    "gt[u_gt] = gt[u_gt].astype(str).str.strip().str.upper()\n",
    "pred[u_pred] = pred[u_pred].astype(str).str.strip().str.upper()\n",
    "\n",
    "# ensure ground truth column exists\n",
    "if 'ground_truth' in gt.columns:\n",
    "    gt['gt_label'] = gt['ground_truth'].astype(int)\n",
    "elif 'label' in gt.columns:\n",
    "    gt['gt_label'] = gt['label'].astype(int)\n",
    "else:\n",
    "    # if answers file is list of users, mark 1 for present, else 0\n",
    "    gt['gt_label'] = 1  # treat rows in answers_r42 as insiders\n",
    "    # to merge properly we want a table of unique users in answers_r42\n",
    "    gt = gt[[u_gt, 'gt_label']].drop_duplicates()\n",
    "\n",
    "# merge predictions with ground truth\n",
    "merged = pd.merge(gt[[u_gt,'gt_label']], pred, left_on=u_gt, right_on=u_pred, how='right', suffixes=('_gt','_pred'))\n",
    "# If right-only rows exist, gt will be NaN -> set to 0 (non-insider)\n",
    "merged['gt_label'] = merged['gt_label'].fillna(0).astype(int)\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "display(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1967a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine predicted label column (anomaly_label) or score column\n",
    "pred_label_col = None\n",
    "pred_score_col = None\n",
    "for c in merged.columns:\n",
    "    if c.lower() in (\"anomaly_label\",\"predicted_label\",\"label_pred\",\"pred_label\"):\n",
    "        pred_label_col = c\n",
    "    if c.lower() in (\"anomaly_score\",\"score\",\"score_pred\",\"anomaly_score\",\"prob\",\"probability\"):\n",
    "        pred_score_col = c\n",
    "\n",
    "print(\"Found predicted label col:\", pred_label_col)\n",
    "print(\"Found predicted score col:\", pred_score_col)\n",
    "\n",
    "# Build y_true and y_pred:\n",
    "y_true = merged['gt_label']\n",
    "\n",
    "if pred_label_col:\n",
    "    y_pred = merged[pred_label_col].fillna(0).astype(int)\n",
    "    print(\"Using predicted label column:\", pred_label_col)\n",
    "elif pred_score_col:\n",
    "    # If we only have continuous score, need threshold (choose 5% default)\n",
    "    scores = merged[pred_score_col].astype(float)\n",
    "    contamination = 0.05\n",
    "    thresh = np.nanpercentile(scores.dropna(), 100 * (1 - contamination)) if scores.max() > scores.min() else scores.median()\n",
    "    # Note: some score conventions are higher == more anomalous, some lower. Inspect distribution:\n",
    "    print(\"Score summary:\", scores.describe())\n",
    "    # Decide direction: if bigger score -> more anomalous (common), or if lower -> more anomalous (iso)\n",
    "    # We'll auto-detect by checking correlation with gt if possible\n",
    "    corr = np.nan\n",
    "    try:\n",
    "        corr = np.corrcoef(scores.fillna(scores.mean()), y_true)[0,1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"Corr(scores, gt) ~\", corr)\n",
    "    # if corr positive, higher score => more anomalous; else invert\n",
    "    if corr >= 0:\n",
    "        y_pred = (scores >= thresh).astype(int)\n",
    "    else:\n",
    "        y_pred = (scores <= thresh).astype(int)\n",
    "    merged['_used_threshold'] = thresh\n",
    "    merged['_used_score'] = scores\n",
    "    print(\"Used threshold:\", thresh)\n",
    "else:\n",
    "    raise KeyError(\"No predicted label or score column found in predictions file. Rename your columns.\")\n",
    "\n",
    "merged['y_true'] = y_true\n",
    "merged['y_pred'] = y_pred\n",
    "print(\"Counts: ground truth:\", y_true.value_counts().to_dict(), \"predicted:\", y_pred.value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Directories & Files\n",
    "# -------------------------\n",
    "BASE_DIR = \".\"\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results_phase5\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "GT_FILE = os.path.join(BASE_DIR, \"answers_r42.csv\")\n",
    "FEATURES_FILE = os.path.join(BASE_DIR, \"user_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Load Data\n",
    "# -------------------------\n",
    "gt = pd.read_csv(GT_FILE)\n",
    "features_df = pd.read_csv(FEATURES_FILE)\n",
    "\n",
    "def find_user_col(df):\n",
    "    for c in df.columns:\n",
    "        if c.lower() in (\"user\",\"user_id\",\"employee\",\"employee_id\",\"id\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "u_gt = find_user_col(gt)\n",
    "u_feat = find_user_col(features_df)\n",
    "if u_gt is None or u_feat is None:\n",
    "    raise KeyError(\"User ID columns not found.\")\n",
    "\n",
    "gt[u_gt] = gt[u_gt].astype(str).str.strip().str.upper()\n",
    "features_df[u_feat] = features_df[u_feat].astype(str).str.strip().str.upper()\n",
    "\n",
    "if 'ground_truth' in gt.columns:\n",
    "    gt['gt_label'] = gt['ground_truth'].astype(int)\n",
    "elif 'label' in gt.columns:\n",
    "    gt['gt_label'] = gt['label'].astype(int)\n",
    "else:\n",
    "    gt['gt_label'] = 1\n",
    "gt = gt[[u_gt,'gt_label']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Merge features with ground truth\n",
    "# -------------------------\n",
    "merged = pd.merge(features_df, gt, left_on=u_feat, right_on=u_gt, how='left')\n",
    "merged['gt_label'] = merged['gt_label'].fillna(0).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Prepare features & labels for training/threshold tuning\n",
    "# -------------------------\n",
    "X = merged.drop(columns=[u_feat,'gt_label'])\n",
    "y = merged['gt_label']\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Small split for threshold tuning\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5️⃣ Apply SMOTE on training\n",
    "# -------------------------\n",
    "smote = SMOTE(sampling_strategy=1.0, random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ Train RandomForest\n",
    "# -------------------------\n",
    "clf = RandomForestClassifier(class_weight='balanced', n_estimators=300, random_state=42)\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04004a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7️⃣ Threshold tuning on small split\n",
    "# -------------------------\n",
    "y_prob = clf.predict_proba(X_test)[:,1]  # Insider probabilities\n",
    "thresholds = np.linspace(0.05, 0.5, 46)\n",
    "best_f1 = 0\n",
    "best_thresh = 0.2\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_prob >= t).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_t, zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"Best threshold from split: {best_thresh:.2f}, F1={best_f1:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 8️⃣ Predict on full merged dataset\n",
    "# -------------------------\n",
    "X_full = X_encoded.copy()\n",
    "y_full_true = merged['gt_label']\n",
    "\n",
    "y_full_prob = clf.predict_proba(X_full)[:,1]\n",
    "y_full_pred = (y_full_prob >= best_thresh).astype(int)\n",
    "\n",
    "cm_full = confusion_matrix(y_full_true, y_full_pred)\n",
    "report_full = classification_report(y_full_true, y_full_pred, target_names=[\"Normal\",\"Insider\"], zero_division=0)\n",
    "\n",
    "print(\"Full Dataset Confusion Matrix:\\n\", cm_full)\n",
    "print(\"\\nFull Dataset Classification Report:\\n\", report_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0684cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 9️⃣ Save report\n",
    "# -------------------------\n",
    "report_path = os.path.join(RESULTS_DIR, \"evaluation_report_full.txt\")\n",
    "with open(report_path, \"w\") as fh:\n",
    "    fh.write(\"Best threshold from split: {:.2f}\\n\\n\".format(best_thresh))\n",
    "    fh.write(\"Confusion Matrix:\\n\")\n",
    "    fh.write(str(cm_full) + \"\\n\\n\")\n",
    "    fh.write(\"Classification Report:\\n\")\n",
    "    fh.write(report_full + \"\\n\")\n",
    "print(\"Saved evaluation report to\", report_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 🔟 Plot confusion matrix heatmap\n",
    "# -------------------------\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Pred Normal\",\"Pred Insider\"],\n",
    "            yticklabels=[\"True Normal\",\"True Insider\"])\n",
    "plt.title(\"Confusion Matrix (Full Dataset, Threshold={:.2f})\".format(best_thresh))\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"confusion_matrix_full.png\"), bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved confusion_matrix_full.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# ROC / PR Curves\n",
    "# -------------------------\n",
    "if len(np.unique(y_true_full)) > 1:\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true_full, scores_full)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "    plt.plot([0,1],[0,1], '--', color='gray')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve - Full Merged\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, \"roc_curve_full_merged.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # PR\n",
    "    precision, recall, _ = precision_recall_curve(y_true_full, scores_full)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(recall, precision, label=f\"PR (AUC = {pr_auc:.3f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve - Full Merged\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, \"pr_curve_full_merged.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Saved ROC/PR plots for full merged dataset\")\n",
    "else:\n",
    "    print(\"Only one class present in ground truth — ROC/PR not computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff84bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Build X,y from merged (drop id/labels)\n",
    "drop_cols = [u_gt, u_pred, 'gt_label', 'y_true', 'y_pred']\n",
    "X_all = merged.drop(columns=[c for c in drop_cols if c in merged.columns], errors='ignore').select_dtypes(include=[np.number]).fillna(0)\n",
    "y_all = merged['gt_label']\n",
    "\n",
    "if X_all.shape[1] > 0 and len(np.unique(y_all)) > 1:\n",
    "    rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_all, y_all)\n",
    "    fi = pd.Series(rf.feature_importances_, index=X_all.columns).sort_values(ascending=False)\n",
    "    display(fi.head(20))\n",
    "    fi.head(20).plot(kind='barh', figsize=(8,6))\n",
    "    plt.title(\"RandomForest feature importances\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR,\"feature_importances.png\"), bbox_inches='tight', dpi=150)\n",
    "    joblib.dump(rf, os.path.join(RESULTS_DIR,\"rf_phase5.joblib\"))\n",
    "    print(\"Saved RF and feature importance plot.\")\n",
    "else:\n",
    "    print(\"Not enough numeric features or only a single label class to train RF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged table (predictions + ground truth + diagnostics)\n",
    "merged.to_csv(os.path.join(RESULTS_DIR, \"phase5_merged_results.csv\"), index=False)\n",
    "print(\"All output saved to:\", RESULTS_DIR)\n",
    "print(\"Files saved:\", os.listdir(RESULTS_DIR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
