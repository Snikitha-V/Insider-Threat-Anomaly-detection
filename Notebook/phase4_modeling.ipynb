{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered features\n",
    "features_df = pd.read_csv(\"user_features.csv\")\n",
    "\n",
    "print(features_df.head())\n",
    "print(features_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"Original columns:\", features_df.columns.tolist())\n",
    "print(\"Data types:\")\n",
    "print(features_df.dtypes)\n",
    "\n",
    "# Drop common identifier/string columns first (so downstream selection is numeric-only)\n",
    "drop_candidates = [c for c in [\"user_id\", \"user\", \"employee_name\", \"employee\"] if c in features_df.columns]\n",
    "if drop_candidates:\n",
    "    print(\"Dropping identifier columns:\", drop_candidates)\n",
    "features_clean = features_df.drop(columns=drop_candidates, errors=\"ignore\")\n",
    "\n",
    "# Select only numeric columns for modeling\n",
    "numeric_columns = features_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumeric columns found: {numeric_columns}\")\n",
    "\n",
    "# Create feature matrix (only numeric columns)\n",
    "X = features_clean[numeric_columns].copy()\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Safety check: ensure no object columns remain\n",
    "obj_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "if obj_cols:\n",
    "    raise ValueError(f\"Non-numeric columns still present in X: {obj_cols}\")\n",
    "\n",
    "# Scale the numeric data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Data successfully scaled!\")\n",
    "print(f\"Scaled data shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.05,  # % of anomalies expected\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "iso_forest.fit(X_scaled)\n",
    "\n",
    "# Predict anomalies\n",
    "features_df[\"anomaly_score\"] = iso_forest.decision_function(X_scaled)\n",
    "features_df[\"anomaly_label\"] = iso_forest.predict(X_scaled)\n",
    "\n",
    "# Convert labels: -1 = anomaly, 1 = normal\n",
    "features_df[\"anomaly_label\"] = features_df[\"anomaly_label\"].map({1: 0, -1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_csv(\"results/anomaly_scores_by_user.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(features_df[\"anomaly_score\"], kde=True)\n",
    "plt.title(\"Distribution of Anomaly Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Count anomalies vs normals\n",
    "print(features_df[\"anomaly_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd64e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"label\" in features_df.columns:\n",
    "    print(classification_report(features_df[\"label\"], features_df[\"anomaly_label\"]))\n",
    "    print(confusion_matrix(features_df[\"label\"], features_df[\"anomaly_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5040481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(iso_forest, \"models/isolation_forest.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11448849",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available columns:\", features_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d549815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da46d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set target\n",
    "target_column = \"anomaly_label\"\n",
    "\n",
    "y = features_df[target_column].astype(int).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop target + IDs from features\n",
    "drop_cols = [\"user\", \"employee_name\", \"user_id\", \"anomaly_label\", \"anomaly_score\"]\n",
    "X = features_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# Keep only numeric features\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "print(\"Final feature columns:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80056a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Scaled feature matrix:\", X_scaled.shape)\n",
    "print(\"Target vector:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train Isolation Forest (or replace with other model)\n",
    "model = IsolationForest(n_estimators=1000, contamination=0.05, random_state=42)\n",
    "model.fit(X_train)\n",
    "\n",
    "# Predict (convert -1 to 1, 1 to 0 to align with labels)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if p == -1 else 0 for p in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87054197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Evaluate\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "insiders_df = pd.read_csv(\"insiders.csv\")\n",
    "print(\"Shape:\", insiders_df.shape)\n",
    "print(\"\\nColumns:\", insiders_df.columns.tolist())\n",
    "print(\"\\nSample data:\")\n",
    "print(insiders_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37596a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv(\"user_features.csv\")\n",
    "insiders_df = pd.read_csv(\"insiders.csv\")\n",
    "print(features_df.shape, insiders_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a081dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[\"user\"] = features_df[\"user\"].astype(str).str.strip().str.upper()\n",
    "insiders_df[\"user\"] = insiders_df[\"user\"].astype(str).str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[\"ground_truth\"] = features_df[\"user\"].isin(insiders_df[\"user\"]).astype(int)\n",
    "print(features_df[\"ground_truth\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- New: Use `answers_r42.csv` as absolute ground truth and run a proper train/val/test pipeline (70/20/10)\n",
    "# This cell replaces use of insiders.csv and fixes leakage by splitting BEFORE fitting scaler/model.\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "# Paths (notebook lives in Notebook/ so use Notebook/ prefixes when running from repo root)\n",
    "features_path = os.path.join('Notebook', 'user_features.csv')\n",
    "answers_path = os.path.join('Notebook', 'answers_r42.csv')\n",
    "results_dir = os.path.join('Notebook', 'results')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Try several candidate locations for the features and answers files so the notebook runs\n",
    "# whether executed from the repo root or from inside the Notebook folder.\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "feature_candidates = [\n",
    "    features_path,\n",
    "    os.path.join(cwd, 'Notebook', 'user_features.csv'),\n",
    "    os.path.join(cwd, 'user_features.csv'),\n",
    "    os.path.join(cwd, '..', 'Notebook', 'user_features.csv'),\n",
    "    os.path.join(cwd, '..', 'user_features.csv')\n",
    "]\n",
    "answer_candidates = [\n",
    "    answers_path,\n",
    "    os.path.join(cwd, 'Notebook', 'answers_r42.csv'),\n",
    "    os.path.join(cwd, 'answers_r42.csv'),\n",
    "    os.path.join(cwd, '..', 'Notebook', 'answers_r42.csv'),\n",
    "    os.path.join(cwd, '..', 'answers_r42.csv')\n",
    "]\n",
    "\n",
    "found_feature = None\n",
    "for p in feature_candidates:\n",
    "    if os.path.exists(p):\n",
    "        features_path = p\n",
    "        found_feature = p\n",
    "        break\n",
    "\n",
    "found_answer = None\n",
    "for p in answer_candidates:\n",
    "    if os.path.exists(p):\n",
    "        answers_path = p\n",
    "        found_answer = p\n",
    "        break\n",
    "\n",
    "if not found_feature or not found_answer:\n",
    "    msg = [f\"CWD: {cwd}\", \"Checked paths for user_features.csv:\"]\n",
    "    msg.extend(feature_candidates)\n",
    "    msg.append(\"Checked paths for answers_r42.csv:\")\n",
    "    msg.extend(answer_candidates)\n",
    "    raise FileNotFoundError(\"Could not locate required input files.\\n\" + \"\\n\".join(map(str, msg)))\n",
    "\n",
    "# Load data\n",
    "features_df = pd.read_csv(features_path)\n",
    "answers_df = pd.read_csv(answers_path)\n",
    "\n",
    "print(f'Loaded features from: {features_path}')\n",
    "print(f'Loaded answers from: {answers_path}')\n",
    "\n",
    "# Normalise user keys\n",
    "if 'user' in features_df.columns and 'user' in answers_df.columns:\n",
    "    features_df['user'] = features_df['user'].astype(str).str.strip().str.upper()\n",
    "    answers_df['user'] = answers_df['user'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Merge or derive ground truth from answers_r42.csv\n",
    "if 'ground_truth' in answers_df.columns:\n",
    "    features_df = features_df.merge(answers_df[['user','ground_truth']], on='user', how='left')\n",
    "elif 'label' in answers_df.columns:\n",
    "    # Accept either 'label' or 'ground_truth' column name in answers file\n",
    "    features_df = features_df.merge(answers_df[['user','label']], on='user', how='left')\n",
    "    features_df['ground_truth'] = features_df['label'].fillna(0).astype(int)\n",
    "else:\n",
    "    # If answers file is just a list of users, mark present users as 1\n",
    "    features_df['ground_truth'] = features_df['user'].isin(answers_df['user']).astype(int)\n",
    "\n",
    "features_df['ground_truth'] = features_df['ground_truth'].fillna(0).astype(int)\n",
    "print('Ground-truth distribution (answers_r42):')\n",
    "print(features_df['ground_truth'].value_counts())\n",
    "\n",
    "# Split: train 70% / val 20% / test 10% (stratify by ground_truth)\n",
    "train_df, temp_df = train_test_split(features_df, test_size=0.3, random_state=42, stratify=features_df['ground_truth'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=1/3, random_state=42, stratify=temp_df['ground_truth'])\n",
    "print('Shapes: train, val, test =', train_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "# Helper to build numeric feature matrix (drop ids & label columns)\n",
    "drop_cols = [\"user\", \"employee_name\", \"user_id\", \"anomaly_label\", \"anomaly_score\", \"label\", \"ground_truth\"]\n",
    "def build_X(df):\n",
    "    X = df.drop(columns=drop_cols, errors='ignore')\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    return X\n",
    "\n",
    "X_train = build_X(train_df)\n",
    "X_val = build_X(val_df)\n",
    "X_test = build_X(test_df)\n",
    "\n",
    "y_train = train_df['ground_truth'].astype(int)\n",
    "y_val = val_df['ground_truth'].astype(int)\n",
    "y_test = test_df['ground_truth'].astype(int)\n",
    "\n",
    "print('Numeric feature counts: train={}, val={}, test={}'.format(X_train.shape[1], X_val.shape[1], X_test.shape[1]))\n",
    "\n",
    "# Fit scaler on train only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- New: Train a lightweight autoencoder on train set (dense AE)\n",
    "try:\n",
    "    from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    encoding_dim = max(8, min(64, input_dim // 2))\n",
    "    ae = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(encoding_dim, activation='relu'),\n",
    "        layers.Dense(max(encoding_dim//2, 4), activation='relu'),\n",
    "        layers.Dense(encoding_dim, activation='relu'),\n",
    "        layers.Dense(input_dim, activation='linear'),\n",
    "    ])\n",
    "    ae.compile(optimizer=optimizers.Adam(1e-3), loss='mse')\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = ae.fit(X_train_scaled, X_train_scaled,\n",
    "                     validation_data=(X_val_scaled, X_val_scaled),\n",
    "                     epochs=200, batch_size=32, verbose=1, callbacks=[es])\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    ae.save('models/autoencoder.keras')\n",
    "    print('Autoencoder trained and saved to models/autoencoder.keras')\n",
    "    # compute AE MSE on val and test\n",
    "    recon_val = ae.predict(X_val_scaled)\n",
    "    ae_mse_val = np.mean(np.square(X_val_scaled - recon_val), axis=1)\n",
    "    recon_test = ae.predict(X_test_scaled)\n",
    "    ae_mse_test = np.mean(np.square(X_test_scaled - recon_test), axis=1)\n",
    "    val_df['ae_mse'] = ae_mse_val\n",
    "    test_df['ae_mse'] = ae_mse_test\n",
    "except Exception as e:\n",
    "    print('Autoencoder training skipped or failed:', e)\n",
    "    ae = None\n",
    "\n",
    "# Train IsolationForest on train set only\n",
    "iso = IsolationForest(n_estimators=1000, contamination=0.05, random_state=42)\n",
    "iso.fit(X_train_scaled)\n",
    "\n",
    "# Compute scores on val and test\n",
    "scores_val = iso.decision_function(X_val_scaled)\n",
    "scores_test = iso.decision_function(X_test_scaled)\n",
    "\n",
    "# Sweep contamination percentiles on validation to pick best F1\n",
    "percs = np.linspace(0.001, 0.5, 500)\n",
    "rows = []\n",
    "for p in percs:\n",
    "    thresh = float(np.nanpercentile(scores_val[~np.isnan(scores_val)], 100 * p))\n",
    "    y_val_pred = (scores_val <= thresh).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val, y_val_pred, average='binary', zero_division=0)\n",
    "    rows.append({'contamination': float(p), 'threshold': thresh, 'precision': float(prec), 'recall': float(rec), 'f1': float(f1), 'predicted_positives': int(np.nansum(y_val_pred))})\n",
    "\n",
    "df_tune = pd.DataFrame(rows)\n",
    "best_row = df_tune.loc[df_tune['f1'].idxmax()]\n",
    "selected_contamination = float(best_row['contamination'])\n",
    "selected_threshold = float(best_row['threshold'])\n",
    "print('\\nBest validation F1:')\n",
    "print(best_row.to_dict())\n",
    "\n",
    "# Save tuning table and selected threshold\n",
    "df_tune.to_csv(os.path.join(results_dir, 'threshold_tuning.csv'), index=False)\n",
    "with open(os.path.join(results_dir, 'selected_threshold.json'), 'w') as fh:\n",
    "    json.dump({'contamination': selected_contamination, 'threshold': selected_threshold, 'f1': float(best_row['f1'])}, fh)\n",
    "print('Saved tuning table and selected threshold to', results_dir)\n",
    "\n",
    "# Evaluate on test set with selected threshold\n",
    "y_test_pred = (scores_test <= selected_threshold).astype(int)\n",
    "print('\\nTest set results (selected threshold applied):')\n",
    "print('Contamination chosen from val: {:.4f}, threshold: {:.6f}'.format(selected_contamination, selected_threshold))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_test_pred))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_test_pred, target_names=['Normal','Insider'], zero_division=0))\n",
    "\n",
    "# Save per-user anomaly scores/labels for the whole dataset (use scaler fitted on train + iso fitted on train)\n",
    "# Ensure the full feature set uses the same columns and ordering as the training features\n",
    "full_X = build_X(features_df)\n",
    "try:\n",
    "    # If X_train is a DataFrame, reindex full_X to match training columns (fill missing with 0)\n",
    "    if hasattr(X_train, 'columns'):\n",
    "        feature_cols = X_train.columns.tolist()\n",
    "        full_X = full_X.reindex(columns=feature_cols, fill_value=0)\n",
    "except Exception:\n",
    "    # Fall back to using full_X as-is if anything unexpected occurs\n",
    "    pass\n",
    "\n",
    "# Now safe to transform with the scaler fitted on the training set\n",
    "full_X_scaled = scaler.transform(full_X)\n",
    "full_scores = iso.decision_function(full_X_scaled)\n",
    "full_df_out = features_df[['user']].copy()\n",
    "full_df_out['anomaly_score'] = full_scores\n",
    "# If autoencoder was trained, compute AE MSE on the full set and attach\n",
    "if 'ae' in globals() and ae is not None:\n",
    "    try:\n",
    "        recon_full = ae.predict(full_X_scaled)\n",
    "        full_df_out['ae_mse'] = np.mean(np.square(full_X_scaled - recon_full), axis=1)\n",
    "    except Exception as e:\n",
    "        print('Failed to compute AE mse on full set:', e)\n",
    "\n",
    "full_df_out['anomaly_label'] = (full_df_out['anomaly_score'] <= selected_threshold).astype(int)\n",
    "full_df_out.to_csv(os.path.join(results_dir, 'anomaly_scores_by_user.csv'), index=False)\n",
    "print('Saved per-user anomaly scores/labels to', os.path.join(results_dir, 'anomaly_scores_by_user.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "\n",
    "# Actual labels (from insider truth file)\n",
    "y_true = features_df[\"ground_truth\"].astype(int)\n",
    "\n",
    "# Load saved predictions (results/anomaly_scores_by_user.csv) and align by user\n",
    "res_path = os.path.join(\"results\", \"anomaly_scores_by_user.csv\")\n",
    "if not os.path.exists(res_path):\n",
    "    raise FileNotFoundError(f\"Results file not found at {res_path}\")\n",
    "res = pd.read_csv(res_path)\n",
    "\n",
    "# Standardize user column if present\n",
    "if 'user' in features_df.columns and 'user' in res.columns:\n",
    "    left = features_df[['user']].copy()\n",
    "    left['user'] = left['user'].astype(str).str.strip().str.upper()\n",
    "    res['user'] = res['user'].astype(str).str.strip().str.upper()\n",
    "    merged = left.merge(res, on='user', how='left')\n",
    "else:\n",
    "    # No user alignment possible â€” try to use order-preserving predictions\n",
    "    merged = res.copy()\n",
    "\n",
    "# Decide predicted labels: prefer anomaly_label column, else infer from anomaly_score using contamination\n",
    "if 'anomaly_label' in merged.columns:\n",
    "    y_pred = merged['anomaly_label'].fillna(0).astype(int)\n",
    "elif 'anomaly_score' in merged.columns:\n",
    "    # Anomaly score from IsolationForest: lower -> more anomalous\n",
    "    scores = merged['anomaly_score'].astype(float)\n",
    "    # default contamination fallback\n",
    "    contamination = 0.05\n",
    "    thresh = np.nanpercentile(scores.dropna(), 100 * contamination)\n",
    "    # predict anomaly if score <= thresh (anomalies are low scores)\n",
    "    y_pred = (scores <= thresh).astype(int).fillna(0)\n",
    "else:\n",
    "    raise KeyError('No anomaly_label or anomaly_score column found in results file.')\n",
    "\n",
    "# If merged includes extra rows or alignment used, ensure lengths match by selecting same index as features_df\n",
    "if 'user' in features_df.columns and 'user' in merged.columns:\n",
    "    # merged length equals features_df\n",
    "    pass\n",
    "else:\n",
    "    # If shapes mismatch, attempt to trim/pad\n",
    "    if len(y_pred) != len(y_true):\n",
    "        raise ValueError(f\"Length mismatch after loading predictions: y_true={len(y_true)}, y_pred={len(y_pred)}. Ensure results file aligns with features_df or contains 'user' column for merging.\")\n",
    "\n",
    "# Print class distribution and diagnostics\n",
    "print(\"Ground-truth counts:\\n\", y_true.value_counts())\n",
    "print(\"Predicted counts:\\n\", pd.Series(y_pred).value_counts())\n",
    "\n",
    "# Generate metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Normal\", \"Insider\"], zero_division=0)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Threat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
