{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2155e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b728523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user  total_logins  unique_days  after_hours  unique_machines  \\\n",
      "0  AAE0190            37           19            0                1   \n",
      "1  AAF0535            37           19            0                1   \n",
      "2  AAF0791            37           19            0                1   \n",
      "3  AAL0706            37           19           19                1   \n",
      "4  AAM0658            37           19           18                1   \n",
      "\n",
      "   usb_count  usb_days          employee_name  user_id   O   C   E   A   N  \n",
      "0        0.0       0.0   August Armando Evans  AAE0190  36  30  14  50  29  \n",
      "1        0.0       0.0  Athena Amelia Foreman  AAF0535  17  21  36  33  31  \n",
      "2        0.0       0.0  Aladdin Abraham Foley  AAF0791  14  40  40  50  34  \n",
      "3        0.0       0.0       April Alika Levy  AAL0706  37  14  28  13  25  \n",
      "4        0.0       0.0       Abel Adam Morton  AAM0658  43  35  37  36  22  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   user             1000 non-null   object \n",
      " 1   total_logins     1000 non-null   int64  \n",
      " 2   unique_days      1000 non-null   int64  \n",
      " 3   after_hours      1000 non-null   int64  \n",
      " 4   unique_machines  1000 non-null   int64  \n",
      " 5   usb_count        1000 non-null   float64\n",
      " 6   usb_days         1000 non-null   float64\n",
      " 7   employee_name    1000 non-null   object \n",
      " 8   user_id          1000 non-null   object \n",
      " 9   O                1000 non-null   int64  \n",
      " 10  C                1000 non-null   int64  \n",
      " 11  E                1000 non-null   int64  \n",
      " 12  A                1000 non-null   int64  \n",
      " 13  N                1000 non-null   int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 109.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load engineered features\n",
    "features_df = pd.read_csv(\"user_features.csv\")\n",
    "\n",
    "print(features_df.head())\n",
    "print(features_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e473fd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['user', 'total_logins', 'unique_days', 'after_hours', 'unique_machines', 'usb_count', 'usb_days', 'employee_name', 'user_id', 'O', 'C', 'E', 'A', 'N']\n",
      "Data types:\n",
      "user                object\n",
      "total_logins         int64\n",
      "unique_days          int64\n",
      "after_hours          int64\n",
      "unique_machines      int64\n",
      "usb_count          float64\n",
      "usb_days           float64\n",
      "employee_name       object\n",
      "user_id             object\n",
      "O                    int64\n",
      "C                    int64\n",
      "E                    int64\n",
      "A                    int64\n",
      "N                    int64\n",
      "dtype: object\n",
      "Dropping identifier columns: ['user_id', 'user', 'employee_name']\n",
      "\n",
      "Numeric columns found: ['total_logins', 'unique_days', 'after_hours', 'unique_machines', 'usb_count', 'usb_days', 'O', 'C', 'E', 'A', 'N']\n",
      "Feature matrix shape: (1000, 11)\n",
      "Data successfully scaled!\n",
      "Scaled data shape: (1000, 11)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"Original columns:\", features_df.columns.tolist())\n",
    "print(\"Data types:\")\n",
    "print(features_df.dtypes)\n",
    "\n",
    "# Drop common identifier/string columns first (so downstream selection is numeric-only)\n",
    "drop_candidates = [c for c in [\"user_id\", \"user\", \"employee_name\", \"employee\"] if c in features_df.columns]\n",
    "if drop_candidates:\n",
    "    print(\"Dropping identifier columns:\", drop_candidates)\n",
    "features_clean = features_df.drop(columns=drop_candidates, errors=\"ignore\")\n",
    "\n",
    "# Select only numeric columns for modeling\n",
    "numeric_columns = features_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumeric columns found: {numeric_columns}\")\n",
    "\n",
    "# Create feature matrix (only numeric columns)\n",
    "X = features_clean[numeric_columns].copy()\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Safety check: ensure no object columns remain\n",
    "obj_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "if obj_cols:\n",
    "    raise ValueError(f\"Non-numeric columns still present in X: {obj_cols}\")\n",
    "\n",
    "# Scale the numeric data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Data successfully scaled!\")\n",
    "print(f\"Scaled data shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d9ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.05,  # % of anomalies expected\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "iso_forest.fit(X_scaled)\n",
    "\n",
    "# Predict anomalies\n",
    "features_df[\"anomaly_score\"] = iso_forest.decision_function(X_scaled)\n",
    "features_df[\"anomaly_label\"] = iso_forest.predict(X_scaled)\n",
    "\n",
    "# Convert labels: -1 = anomaly, 1 = normal\n",
    "features_df[\"anomaly_label\"] = features_df[\"anomaly_label\"].map({1: 0, -1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f31139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_csv(\"results/anomaly_scores_by_user.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f09b226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly_label\n",
      "0    950\n",
      "1     50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sns.histplot(features_df[\"anomaly_score\"], kde=True)\n",
    "plt.title(\"Distribution of Anomaly Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Count anomalies vs normals\n",
    "print(features_df[\"anomaly_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfd64e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"label\" in features_df.columns:\n",
    "    print(classification_report(features_df[\"label\"], features_df[\"anomaly_label\"]))\n",
    "    print(confusion_matrix(features_df[\"label\"], features_df[\"anomaly_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5040481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/isolation_forest.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(iso_forest, \"models/isolation_forest.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11448849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['user', 'total_logins', 'unique_days', 'after_hours', 'unique_machines', 'usb_count', 'usb_days', 'employee_name', 'user_id', 'O', 'C', 'E', 'A', 'N', 'anomaly_score', 'anomaly_label']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns:\", features_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d549815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4da46d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set target\n",
    "target_column = \"anomaly_label\"\n",
    "\n",
    "y = features_df[target_column].astype(int).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b38dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature columns: ['total_logins', 'unique_days', 'after_hours', 'unique_machines', 'usb_count', 'usb_days', 'O', 'C', 'E', 'A', 'N']\n"
     ]
    }
   ],
   "source": [
    "# 2. Drop target + IDs from features\n",
    "drop_cols = [\"user\", \"employee_name\", \"user_id\", \"anomaly_label\", \"anomaly_score\"]\n",
    "X = features_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# Keep only numeric features\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "print(\"Final feature columns:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80056a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature matrix: (1000, 11)\n",
      "Target vector: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# 3. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Scaled feature matrix:\", X_scaled.shape)\n",
    "print(\"Target vector:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf5c8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db02aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train Isolation Forest (or replace with other model)\n",
    "model = IsolationForest(n_estimators=1000, contamination=0.05, random_state=42)\n",
    "model.fit(X_train)\n",
    "\n",
    "# Predict (convert -1 to 1, 1 to 0 to align with labels)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if p == -1 else 0 for p in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87054197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[284   1]\n",
      " [  0  15]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       285\n",
      "           1       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           1.00       300\n",
      "   macro avg       0.97      1.00      0.98       300\n",
      "weighted avg       1.00      1.00      1.00       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d4d1700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (191, 6)\n",
      "\n",
      "Columns: ['dataset', 'scenario', 'details', 'user', 'start', 'end']\n",
      "\n",
      "Sample data:\n",
      "   dataset  scenario     details     user                start  \\\n",
      "0      2.0         1      r2.csv  ONS0995     3/6/2010 1:41:56   \n",
      "1      3.1         1  r3.1-1.csv  CSF0929  07/01/2010 01:24:58   \n",
      "2      3.1         2  r3.1-2.csv  CCH0959  08/02/2010 10:34:31   \n",
      "3      3.2         1  r3.2-1.csv  RCW0822  09/29/2010 21:10:27   \n",
      "4      3.2         2  r3.2-2.csv  JCE0258  07/12/2010 08:16:02   \n",
      "\n",
      "                   end  \n",
      "0    3/20/2010 8:10:12  \n",
      "1  07/16/2010 06:52:00  \n",
      "2  09/30/2010 15:04:03  \n",
      "3  10/15/2010 06:34:52  \n",
      "4  09/03/2010 16:16:29  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "insiders_df = pd.read_csv(\"insiders.csv\")\n",
    "print(\"Shape:\", insiders_df.shape)\n",
    "print(\"\\nColumns:\", insiders_df.columns.tolist())\n",
    "print(\"\\nSample data:\")\n",
    "print(insiders_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37596a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 14) (191, 6)\n"
     ]
    }
   ],
   "source": [
    "features_df = pd.read_csv(\"user_features.csv\")\n",
    "insiders_df = pd.read_csv(\"insiders.csv\")\n",
    "print(features_df.shape, insiders_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a081dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[\"user\"] = features_df[\"user\"].astype(str).str.strip().str.upper()\n",
    "insiders_df[\"user\"] = insiders_df[\"user\"].astype(str).str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d322b59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth\n",
      "0    930\n",
      "1     70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features_df[\"ground_truth\"] = features_df[\"user\"].isin(insiders_df[\"user\"]).astype(int)\n",
    "print(features_df[\"ground_truth\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4dcc346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features from: c:\\Users\\sniki\\OneDrive\\Desktop\\Insider Threat Anamoly Detection\\Notebook\\user_features.csv\n",
      "Loaded answers from: c:\\Users\\sniki\\OneDrive\\Desktop\\Insider Threat Anamoly Detection\\Notebook\\answers_r42.csv\n",
      "Ground-truth distribution (answers_r42):\n",
      "ground_truth\n",
      "0    930\n",
      "1     70\n",
      "Name: count, dtype: int64\n",
      "Shapes: train, val, test = (700, 16) (200, 16) (100, 16)\n",
      "Numeric feature counts: train=11, val=11, test=11\n",
      "Epoch 1/200\n",
      "Epoch 1/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.8568 - val_loss: 0.7188\n",
      "Epoch 2/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.8568 - val_loss: 0.7188\n",
      "Epoch 2/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8132 - val_loss: 0.6942\n",
      "Epoch 3/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8132 - val_loss: 0.6942\n",
      "Epoch 3/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7896 - val_loss: 0.6782\n",
      "Epoch 4/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7896 - val_loss: 0.6782\n",
      "Epoch 4/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7711 - val_loss: 0.6602\n",
      "Epoch 5/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7711 - val_loss: 0.6602\n",
      "Epoch 5/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7495 - val_loss: 0.6408\n",
      "Epoch 6/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7495 - val_loss: 0.6408\n",
      "Epoch 6/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7233 - val_loss: 0.6163\n",
      "Epoch 7/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7233 - val_loss: 0.6163\n",
      "Epoch 7/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6926 - val_loss: 0.5898\n",
      "Epoch 8/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6926 - val_loss: 0.5898\n",
      "Epoch 8/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6593 - val_loss: 0.5626\n",
      "Epoch 9/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6593 - val_loss: 0.5626\n",
      "Epoch 9/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6238 - val_loss: 0.5385\n",
      "Epoch 10/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6238 - val_loss: 0.5385\n",
      "Epoch 10/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5965 - val_loss: 0.5187\n",
      "Epoch 11/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5965 - val_loss: 0.5187\n",
      "Epoch 11/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5722 - val_loss: 0.5044\n",
      "Epoch 12/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5722 - val_loss: 0.5044\n",
      "Epoch 12/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5522 - val_loss: 0.4903\n",
      "Epoch 13/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5522 - val_loss: 0.4903\n",
      "Epoch 13/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5320 - val_loss: 0.4769\n",
      "Epoch 14/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5320 - val_loss: 0.4769\n",
      "Epoch 14/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5129 - val_loss: 0.4650\n",
      "Epoch 15/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5129 - val_loss: 0.4650\n",
      "Epoch 15/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4961 - val_loss: 0.4547\n",
      "Epoch 16/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4961 - val_loss: 0.4547\n",
      "Epoch 16/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4811 - val_loss: 0.4456\n",
      "Epoch 17/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4811 - val_loss: 0.4456\n",
      "Epoch 17/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4677 - val_loss: 0.4369\n",
      "Epoch 18/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4677 - val_loss: 0.4369\n",
      "Epoch 18/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4566 - val_loss: 0.4307\n",
      "Epoch 19/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4566 - val_loss: 0.4307\n",
      "Epoch 19/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4467 - val_loss: 0.4240\n",
      "Epoch 20/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4467 - val_loss: 0.4240\n",
      "Epoch 20/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4381 - val_loss: 0.4180\n",
      "Epoch 21/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4381 - val_loss: 0.4180\n",
      "Epoch 21/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4305 - val_loss: 0.4134\n",
      "Epoch 22/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4305 - val_loss: 0.4134\n",
      "Epoch 22/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4241 - val_loss: 0.4089\n",
      "Epoch 23/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4241 - val_loss: 0.4089\n",
      "Epoch 23/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4182 - val_loss: 0.4050\n",
      "Epoch 24/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4182 - val_loss: 0.4050\n",
      "Epoch 24/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4127 - val_loss: 0.4010\n",
      "Epoch 25/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4127 - val_loss: 0.4010\n",
      "Epoch 25/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4087 - val_loss: 0.3970\n",
      "Epoch 26/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4087 - val_loss: 0.3970\n",
      "Epoch 26/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4042 - val_loss: 0.3951\n",
      "Epoch 27/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4042 - val_loss: 0.3951\n",
      "Epoch 27/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4000 - val_loss: 0.3910\n",
      "Epoch 28/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4000 - val_loss: 0.3910\n",
      "Epoch 28/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3963 - val_loss: 0.3878\n",
      "Epoch 29/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3963 - val_loss: 0.3878\n",
      "Epoch 29/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3923 - val_loss: 0.3838\n",
      "Epoch 30/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3923 - val_loss: 0.3838\n",
      "Epoch 30/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3886 - val_loss: 0.3807\n",
      "Epoch 31/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3886 - val_loss: 0.3807\n",
      "Epoch 31/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3852 - val_loss: 0.3768\n",
      "Epoch 32/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3852 - val_loss: 0.3768\n",
      "Epoch 32/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3810 - val_loss: 0.3727\n",
      "Epoch 33/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3810 - val_loss: 0.3727\n",
      "Epoch 33/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3768 - val_loss: 0.3684\n",
      "Epoch 34/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3768 - val_loss: 0.3684\n",
      "Epoch 34/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3724 - val_loss: 0.3639\n",
      "Epoch 35/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3724 - val_loss: 0.3639\n",
      "Epoch 35/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3677 - val_loss: 0.3588\n",
      "Epoch 36/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3677 - val_loss: 0.3588\n",
      "Epoch 36/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3625 - val_loss: 0.3543\n",
      "Epoch 37/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3625 - val_loss: 0.3543\n",
      "Epoch 37/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3568 - val_loss: 0.3478\n",
      "Epoch 38/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3568 - val_loss: 0.3478\n",
      "Epoch 38/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3509 - val_loss: 0.3417\n",
      "Epoch 39/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3509 - val_loss: 0.3417\n",
      "Epoch 39/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3443 - val_loss: 0.3354\n",
      "Epoch 40/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3443 - val_loss: 0.3354\n",
      "Epoch 40/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3379 - val_loss: 0.3296\n",
      "Epoch 41/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3379 - val_loss: 0.3296\n",
      "Epoch 41/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3315 - val_loss: 0.3230\n",
      "Epoch 42/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3315 - val_loss: 0.3230\n",
      "Epoch 42/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3251 - val_loss: 0.3185\n",
      "Epoch 43/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3251 - val_loss: 0.3185\n",
      "Epoch 43/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3201 - val_loss: 0.3133\n",
      "Epoch 44/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3201 - val_loss: 0.3133\n",
      "Epoch 44/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3150 - val_loss: 0.3090\n",
      "Epoch 45/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3150 - val_loss: 0.3090\n",
      "Epoch 45/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3111 - val_loss: 0.3045\n",
      "Epoch 46/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3111 - val_loss: 0.3045\n",
      "Epoch 46/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3075 - val_loss: 0.3010\n",
      "Epoch 47/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3075 - val_loss: 0.3010\n",
      "Epoch 47/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3046 - val_loss: 0.2975\n",
      "Epoch 48/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3046 - val_loss: 0.2975\n",
      "Epoch 48/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3015 - val_loss: 0.2941\n",
      "Epoch 49/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3015 - val_loss: 0.2941\n",
      "Epoch 49/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2991 - val_loss: 0.2916\n",
      "Epoch 50/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2991 - val_loss: 0.2916\n",
      "Epoch 50/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2966 - val_loss: 0.2884\n",
      "Epoch 51/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2966 - val_loss: 0.2884\n",
      "Epoch 51/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2942 - val_loss: 0.2862\n",
      "Epoch 52/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2942 - val_loss: 0.2862\n",
      "Epoch 52/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2920 - val_loss: 0.2832\n",
      "Epoch 53/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2920 - val_loss: 0.2832\n",
      "Epoch 53/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2896 - val_loss: 0.2809\n",
      "Epoch 54/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2896 - val_loss: 0.2809\n",
      "Epoch 54/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2872 - val_loss: 0.2781\n",
      "Epoch 55/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2872 - val_loss: 0.2781\n",
      "Epoch 55/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2852 - val_loss: 0.2746\n",
      "Epoch 56/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2852 - val_loss: 0.2746\n",
      "Epoch 56/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2826 - val_loss: 0.2718\n",
      "Epoch 57/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2826 - val_loss: 0.2718\n",
      "Epoch 57/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2798 - val_loss: 0.2688\n",
      "Epoch 58/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2798 - val_loss: 0.2688\n",
      "Epoch 58/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2767 - val_loss: 0.2651\n",
      "Epoch 59/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2767 - val_loss: 0.2651\n",
      "Epoch 59/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2735 - val_loss: 0.2613\n",
      "Epoch 60/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2735 - val_loss: 0.2613\n",
      "Epoch 60/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2693 - val_loss: 0.2574\n",
      "Epoch 61/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2693 - val_loss: 0.2574\n",
      "Epoch 61/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2657 - val_loss: 0.2529\n",
      "Epoch 62/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2657 - val_loss: 0.2529\n",
      "Epoch 62/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2614 - val_loss: 0.2489\n",
      "Epoch 63/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2614 - val_loss: 0.2489\n",
      "Epoch 63/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2574 - val_loss: 0.2451\n",
      "Epoch 64/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2574 - val_loss: 0.2451\n",
      "Epoch 64/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2539 - val_loss: 0.2423\n",
      "Epoch 65/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2539 - val_loss: 0.2423\n",
      "Epoch 65/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2508 - val_loss: 0.2389\n",
      "Epoch 66/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2508 - val_loss: 0.2389\n",
      "Epoch 66/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2477 - val_loss: 0.2357\n",
      "Epoch 67/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2477 - val_loss: 0.2357\n",
      "Epoch 67/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2449 - val_loss: 0.2329\n",
      "Epoch 68/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2449 - val_loss: 0.2329\n",
      "Epoch 68/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2425 - val_loss: 0.2305\n",
      "Epoch 69/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2425 - val_loss: 0.2305\n",
      "Epoch 69/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2396 - val_loss: 0.2279\n",
      "Epoch 70/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2396 - val_loss: 0.2279\n",
      "Epoch 70/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2371 - val_loss: 0.2256\n",
      "Epoch 71/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2371 - val_loss: 0.2256\n",
      "Epoch 71/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2349 - val_loss: 0.2233\n",
      "Epoch 72/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2349 - val_loss: 0.2233\n",
      "Epoch 72/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2333 - val_loss: 0.2213\n",
      "Epoch 73/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2333 - val_loss: 0.2213\n",
      "Epoch 73/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2310 - val_loss: 0.2191\n",
      "Epoch 74/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2310 - val_loss: 0.2191\n",
      "Epoch 74/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2292 - val_loss: 0.2178\n",
      "Epoch 75/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2292 - val_loss: 0.2178\n",
      "Epoch 75/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2278 - val_loss: 0.2158\n",
      "Epoch 76/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2278 - val_loss: 0.2158\n",
      "Epoch 76/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2262 - val_loss: 0.2143\n",
      "Epoch 77/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2262 - val_loss: 0.2143\n",
      "Epoch 77/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2241 - val_loss: 0.2129\n",
      "Epoch 78/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2241 - val_loss: 0.2129\n",
      "Epoch 78/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2229 - val_loss: 0.2112\n",
      "Epoch 79/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2229 - val_loss: 0.2112\n",
      "Epoch 79/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2216 - val_loss: 0.2102\n",
      "Epoch 80/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2216 - val_loss: 0.2102\n",
      "Epoch 80/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2200 - val_loss: 0.2088\n",
      "Epoch 81/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2200 - val_loss: 0.2088\n",
      "Epoch 81/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2190 - val_loss: 0.2073\n",
      "Epoch 82/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2190 - val_loss: 0.2073\n",
      "Epoch 82/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2177 - val_loss: 0.2064\n",
      "Epoch 83/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2177 - val_loss: 0.2064\n",
      "Epoch 83/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2164 - val_loss: 0.2054\n",
      "Epoch 84/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2164 - val_loss: 0.2054\n",
      "Epoch 84/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2157 - val_loss: 0.2039\n",
      "Epoch 85/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2157 - val_loss: 0.2039\n",
      "Epoch 85/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2147 - val_loss: 0.2027\n",
      "Epoch 86/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2147 - val_loss: 0.2027\n",
      "Epoch 86/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2137 - val_loss: 0.2027\n",
      "Epoch 87/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2137 - val_loss: 0.2027\n",
      "Epoch 87/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2129 - val_loss: 0.2010\n",
      "Epoch 88/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2129 - val_loss: 0.2010\n",
      "Epoch 88/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2118 - val_loss: 0.2000\n",
      "Epoch 89/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2118 - val_loss: 0.2000\n",
      "Epoch 89/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2115 - val_loss: 0.1997\n",
      "Epoch 90/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2115 - val_loss: 0.1997\n",
      "Epoch 90/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2101 - val_loss: 0.1990\n",
      "Epoch 91/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2101 - val_loss: 0.1990\n",
      "Epoch 91/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2095 - val_loss: 0.1982\n",
      "Epoch 92/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2095 - val_loss: 0.1982\n",
      "Epoch 92/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2087 - val_loss: 0.1973\n",
      "Epoch 93/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2087 - val_loss: 0.1973\n",
      "Epoch 93/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2081 - val_loss: 0.1971\n",
      "Epoch 94/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2081 - val_loss: 0.1971\n",
      "Epoch 94/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2072 - val_loss: 0.1959\n",
      "Epoch 95/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2072 - val_loss: 0.1959\n",
      "Epoch 95/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2068 - val_loss: 0.1955\n",
      "Epoch 96/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2068 - val_loss: 0.1955\n",
      "Epoch 96/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2064 - val_loss: 0.1944\n",
      "Epoch 97/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2064 - val_loss: 0.1944\n",
      "Epoch 97/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2053 - val_loss: 0.1944\n",
      "Epoch 98/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2053 - val_loss: 0.1944\n",
      "Epoch 98/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2053 - val_loss: 0.1939\n",
      "Epoch 99/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2053 - val_loss: 0.1939\n",
      "Epoch 99/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2044 - val_loss: 0.1936\n",
      "Epoch 100/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2044 - val_loss: 0.1936\n",
      "Epoch 100/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2040 - val_loss: 0.1924\n",
      "Epoch 101/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2040 - val_loss: 0.1924\n",
      "Epoch 101/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2033 - val_loss: 0.1926\n",
      "Epoch 102/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2033 - val_loss: 0.1926\n",
      "Epoch 102/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2030 - val_loss: 0.1924\n",
      "Epoch 103/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2030 - val_loss: 0.1924\n",
      "Epoch 103/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2026 - val_loss: 0.1917\n",
      "Epoch 104/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2026 - val_loss: 0.1917\n",
      "Epoch 104/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2022 - val_loss: 0.1916\n",
      "Epoch 105/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2022 - val_loss: 0.1916\n",
      "Epoch 105/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2016 - val_loss: 0.1907\n",
      "Epoch 106/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2016 - val_loss: 0.1907\n",
      "Epoch 106/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2012 - val_loss: 0.1907\n",
      "Epoch 107/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2012 - val_loss: 0.1907\n",
      "Epoch 107/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2004 - val_loss: 0.1900\n",
      "Epoch 108/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2004 - val_loss: 0.1900\n",
      "Epoch 108/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1999 - val_loss: 0.1893\n",
      "Epoch 109/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1999 - val_loss: 0.1893\n",
      "Epoch 109/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1998 - val_loss: 0.1894\n",
      "Epoch 110/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1998 - val_loss: 0.1894\n",
      "Epoch 110/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1992 - val_loss: 0.1892\n",
      "Epoch 111/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1992 - val_loss: 0.1892\n",
      "Epoch 111/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1991 - val_loss: 0.1881\n",
      "Epoch 112/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1991 - val_loss: 0.1881\n",
      "Epoch 112/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1987 - val_loss: 0.1875\n",
      "Epoch 113/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1987 - val_loss: 0.1875\n",
      "Epoch 113/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1980 - val_loss: 0.1878\n",
      "Epoch 114/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1980 - val_loss: 0.1878\n",
      "Epoch 114/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1979 - val_loss: 0.1876\n",
      "Epoch 115/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1979 - val_loss: 0.1876\n",
      "Epoch 115/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1977 - val_loss: 0.1869\n",
      "Epoch 116/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1977 - val_loss: 0.1869\n",
      "Epoch 116/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1971 - val_loss: 0.1868\n",
      "Epoch 117/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1971 - val_loss: 0.1868\n",
      "Epoch 117/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1971 - val_loss: 0.1866\n",
      "Epoch 118/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1971 - val_loss: 0.1866\n",
      "Epoch 118/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1962 - val_loss: 0.1864\n",
      "Epoch 119/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1962 - val_loss: 0.1864\n",
      "Epoch 119/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1960 - val_loss: 0.1856\n",
      "Epoch 120/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1960 - val_loss: 0.1856\n",
      "Epoch 120/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1956 - val_loss: 0.1856\n",
      "Epoch 121/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1956 - val_loss: 0.1856\n",
      "Epoch 121/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1955 - val_loss: 0.1857\n",
      "Epoch 122/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1955 - val_loss: 0.1857\n",
      "Epoch 122/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1950 - val_loss: 0.1848\n",
      "Epoch 123/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1950 - val_loss: 0.1848\n",
      "Epoch 123/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1944 - val_loss: 0.1843\n",
      "Epoch 124/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1944 - val_loss: 0.1843\n",
      "Epoch 124/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1944 - val_loss: 0.1848\n",
      "Epoch 125/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1944 - val_loss: 0.1848\n",
      "Epoch 125/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1940 - val_loss: 0.1837\n",
      "Epoch 126/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1940 - val_loss: 0.1837\n",
      "Epoch 126/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1933 - val_loss: 0.1838\n",
      "Epoch 127/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1933 - val_loss: 0.1838\n",
      "Epoch 127/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1933 - val_loss: 0.1830\n",
      "Epoch 128/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1933 - val_loss: 0.1830\n",
      "Epoch 128/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1926 - val_loss: 0.1833\n",
      "Epoch 129/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1926 - val_loss: 0.1833\n",
      "Epoch 129/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1922 - val_loss: 0.1830\n",
      "Epoch 130/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1922 - val_loss: 0.1830\n",
      "Epoch 130/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1921 - val_loss: 0.1827\n",
      "Epoch 131/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1921 - val_loss: 0.1827\n",
      "Epoch 131/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1914 - val_loss: 0.1823\n",
      "Epoch 132/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1914 - val_loss: 0.1823\n",
      "Epoch 132/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1913 - val_loss: 0.1820\n",
      "Epoch 133/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1913 - val_loss: 0.1820\n",
      "Epoch 133/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1906 - val_loss: 0.1813\n",
      "Epoch 134/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1906 - val_loss: 0.1813\n",
      "Epoch 134/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1911 - val_loss: 0.1815\n",
      "Epoch 135/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1911 - val_loss: 0.1815\n",
      "Epoch 135/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1912 - val_loss: 0.1815\n",
      "Epoch 136/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1912 - val_loss: 0.1815\n",
      "Epoch 136/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1901 - val_loss: 0.1812\n",
      "Epoch 137/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1901 - val_loss: 0.1812\n",
      "Epoch 137/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1899 - val_loss: 0.1804\n",
      "Epoch 138/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1899 - val_loss: 0.1804\n",
      "Epoch 138/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1893 - val_loss: 0.1811\n",
      "Epoch 139/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1893 - val_loss: 0.1811\n",
      "Epoch 139/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1890 - val_loss: 0.1804\n",
      "Epoch 140/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1890 - val_loss: 0.1804\n",
      "Epoch 140/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1891 - val_loss: 0.1804\n",
      "Epoch 141/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1891 - val_loss: 0.1804\n",
      "Epoch 141/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1882 - val_loss: 0.1802\n",
      "Epoch 142/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1882 - val_loss: 0.1802\n",
      "Epoch 142/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1884 - val_loss: 0.1799\n",
      "Epoch 143/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1884 - val_loss: 0.1799\n",
      "Epoch 143/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1877 - val_loss: 0.1797\n",
      "Epoch 144/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1877 - val_loss: 0.1797\n",
      "Epoch 144/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1876 - val_loss: 0.1796\n",
      "Epoch 145/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1876 - val_loss: 0.1796\n",
      "Epoch 145/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1872 - val_loss: 0.1789\n",
      "Epoch 146/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1872 - val_loss: 0.1789\n",
      "Epoch 146/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1869 - val_loss: 0.1793\n",
      "Epoch 147/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1869 - val_loss: 0.1793\n",
      "Epoch 147/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1876 - val_loss: 0.1799\n",
      "Epoch 148/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1876 - val_loss: 0.1799\n",
      "Epoch 148/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1866 - val_loss: 0.1788\n",
      "Epoch 149/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1866 - val_loss: 0.1788\n",
      "Epoch 149/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1861 - val_loss: 0.1789\n",
      "Epoch 150/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1861 - val_loss: 0.1789\n",
      "Epoch 150/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1859 - val_loss: 0.1789\n",
      "Epoch 151/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1859 - val_loss: 0.1789\n",
      "Epoch 151/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1857 - val_loss: 0.1787\n",
      "Epoch 152/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1857 - val_loss: 0.1787\n",
      "Epoch 152/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1855 - val_loss: 0.1783\n",
      "Epoch 153/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1855 - val_loss: 0.1783\n",
      "Epoch 153/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1850 - val_loss: 0.1785\n",
      "Epoch 154/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1850 - val_loss: 0.1785\n",
      "Epoch 154/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1851 - val_loss: 0.1779\n",
      "Epoch 155/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1851 - val_loss: 0.1779\n",
      "Epoch 155/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1847 - val_loss: 0.1780\n",
      "Epoch 156/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1847 - val_loss: 0.1780\n",
      "Epoch 156/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1845 - val_loss: 0.1782\n",
      "Epoch 157/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1845 - val_loss: 0.1782\n",
      "Epoch 157/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1842 - val_loss: 0.1782\n",
      "Epoch 158/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1842 - val_loss: 0.1782\n",
      "Epoch 158/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1841 - val_loss: 0.1779\n",
      "Epoch 159/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1841 - val_loss: 0.1779\n",
      "Epoch 159/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1839 - val_loss: 0.1777\n",
      "Epoch 160/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1839 - val_loss: 0.1777\n",
      "Epoch 160/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1835 - val_loss: 0.1773\n",
      "Epoch 161/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1835 - val_loss: 0.1773\n",
      "Epoch 161/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1831 - val_loss: 0.1779\n",
      "Epoch 162/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1831 - val_loss: 0.1779\n",
      "Epoch 162/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1831 - val_loss: 0.1773\n",
      "Epoch 163/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1831 - val_loss: 0.1773\n",
      "Epoch 163/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1826 - val_loss: 0.1774\n",
      "Epoch 164/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1826 - val_loss: 0.1774\n",
      "Epoch 164/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1824 - val_loss: 0.1770\n",
      "Epoch 165/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1824 - val_loss: 0.1770\n",
      "Epoch 165/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1820 - val_loss: 0.1771\n",
      "Epoch 166/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1820 - val_loss: 0.1771\n",
      "Epoch 166/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1823 - val_loss: 0.1769\n",
      "Epoch 167/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1823 - val_loss: 0.1769\n",
      "Epoch 167/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1815 - val_loss: 0.1765\n",
      "Epoch 168/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1815 - val_loss: 0.1765\n",
      "Epoch 168/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1816 - val_loss: 0.1771\n",
      "Epoch 169/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1816 - val_loss: 0.1771\n",
      "Epoch 169/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1811 - val_loss: 0.1762\n",
      "Epoch 170/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1811 - val_loss: 0.1762\n",
      "Epoch 170/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1808 - val_loss: 0.1765\n",
      "Epoch 171/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1808 - val_loss: 0.1765\n",
      "Epoch 171/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1803 - val_loss: 0.1761\n",
      "Epoch 172/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1803 - val_loss: 0.1761\n",
      "Epoch 172/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1800 - val_loss: 0.1761\n",
      "Epoch 173/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1800 - val_loss: 0.1761\n",
      "Epoch 173/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1798 - val_loss: 0.1761\n",
      "Epoch 174/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1798 - val_loss: 0.1761\n",
      "Epoch 174/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1796 - val_loss: 0.1754\n",
      "Epoch 175/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1796 - val_loss: 0.1754\n",
      "Epoch 175/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1798 - val_loss: 0.1766\n",
      "Epoch 176/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1798 - val_loss: 0.1766\n",
      "Epoch 176/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1796 - val_loss: 0.1755\n",
      "Epoch 177/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1796 - val_loss: 0.1755\n",
      "Epoch 177/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1792 - val_loss: 0.1754\n",
      "Epoch 178/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1792 - val_loss: 0.1754\n",
      "Epoch 178/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1788 - val_loss: 0.1750\n",
      "Epoch 179/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1788 - val_loss: 0.1750\n",
      "Epoch 179/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1790 - val_loss: 0.1767\n",
      "Epoch 180/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1790 - val_loss: 0.1767\n",
      "Epoch 180/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1786 - val_loss: 0.1748\n",
      "Epoch 181/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1786 - val_loss: 0.1748\n",
      "Epoch 181/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1784 - val_loss: 0.1749\n",
      "Epoch 182/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1784 - val_loss: 0.1749\n",
      "Epoch 182/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1781 - val_loss: 0.1748\n",
      "Epoch 183/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1781 - val_loss: 0.1748\n",
      "Epoch 183/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1775 - val_loss: 0.1747\n",
      "Epoch 184/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1775 - val_loss: 0.1747\n",
      "Epoch 184/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1774 - val_loss: 0.1743\n",
      "Epoch 185/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1774 - val_loss: 0.1743\n",
      "Epoch 185/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1773 - val_loss: 0.1746\n",
      "Epoch 186/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1773 - val_loss: 0.1746\n",
      "Epoch 186/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1770 - val_loss: 0.1743\n",
      "Epoch 187/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1770 - val_loss: 0.1743\n",
      "Epoch 187/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1768 - val_loss: 0.1740\n",
      "Epoch 188/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1768 - val_loss: 0.1740\n",
      "Epoch 188/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1769 - val_loss: 0.1738\n",
      "Epoch 189/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1769 - val_loss: 0.1738\n",
      "Epoch 189/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1767 - val_loss: 0.1741\n",
      "Epoch 190/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1767 - val_loss: 0.1741\n",
      "Epoch 190/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1760 - val_loss: 0.1737\n",
      "Epoch 191/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1760 - val_loss: 0.1737\n",
      "Epoch 191/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1758 - val_loss: 0.1734\n",
      "Epoch 192/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1758 - val_loss: 0.1734\n",
      "Epoch 192/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1756 - val_loss: 0.1734\n",
      "Epoch 193/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1756 - val_loss: 0.1734\n",
      "Epoch 193/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1753 - val_loss: 0.1733\n",
      "Epoch 194/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1753 - val_loss: 0.1733\n",
      "Epoch 194/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1753 - val_loss: 0.1731\n",
      "Epoch 195/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1753 - val_loss: 0.1731\n",
      "Epoch 195/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1753 - val_loss: 0.1732\n",
      "Epoch 196/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1753 - val_loss: 0.1732\n",
      "Epoch 196/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1753 - val_loss: 0.1739\n",
      "Epoch 197/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1753 - val_loss: 0.1739\n",
      "Epoch 197/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1749 - val_loss: 0.1722\n",
      "Epoch 198/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1749 - val_loss: 0.1722\n",
      "Epoch 198/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1748 - val_loss: 0.1726\n",
      "Epoch 199/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1748 - val_loss: 0.1726\n",
      "Epoch 199/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1743 - val_loss: 0.1733\n",
      "Epoch 200/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1743 - val_loss: 0.1733\n",
      "Epoch 200/200\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1738 - val_loss: 0.1726\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1738 - val_loss: 0.1726\n",
      "Autoencoder trained and saved to models/autoencoder.keras\n",
      "Autoencoder trained and saved to models/autoencoder.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\n",
      "Best validation F1:\n",
      "{'contamination': 0.031, 'threshold': -0.007357755661818867, 'precision': 0.42857142857142855, 'recall': 0.21428571428571427, 'f1': 0.2857142857142857, 'predicted_positives': 7.0}\n",
      "Saved tuning table and selected threshold to Notebook\\results\n",
      "\n",
      "Test set results (selected threshold applied):\n",
      "Contamination chosen from val: 0.0310, threshold: -0.007358\n",
      "\n",
      "Confusion Matrix:\n",
      " [[93  0]\n",
      " [ 7  0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.93      1.00      0.96        93\n",
      "     Insider       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.47      0.50      0.48       100\n",
      "weighted avg       0.86      0.93      0.90       100\n",
      "\n",
      "\u001b[1m 1/32\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Best validation F1:\n",
      "{'contamination': 0.031, 'threshold': -0.007357755661818867, 'precision': 0.42857142857142855, 'recall': 0.21428571428571427, 'f1': 0.2857142857142857, 'predicted_positives': 7.0}\n",
      "Saved tuning table and selected threshold to Notebook\\results\n",
      "\n",
      "Test set results (selected threshold applied):\n",
      "Contamination chosen from val: 0.0310, threshold: -0.007358\n",
      "\n",
      "Confusion Matrix:\n",
      " [[93  0]\n",
      " [ 7  0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.93      1.00      0.96        93\n",
      "     Insider       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.47      0.50      0.48       100\n",
      "weighted avg       0.86      0.93      0.90       100\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Saved per-user anomaly scores/labels to Notebook\\results\\anomaly_scores_by_user.csv\n",
      "Saved per-user anomaly scores/labels to Notebook\\results\\anomaly_scores_by_user.csv\n"
     ]
    }
   ],
   "source": [
    "# --- New: Use `answers_r42.csv` as absolute ground truth and run a proper train/val/test pipeline (70/20/10)\n",
    "# This cell replaces use of insiders.csv and fixes leakage by splitting BEFORE fitting scaler/model.\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "# Paths (notebook lives in Notebook/ so use Notebook/ prefixes when running from repo root)\n",
    "features_path = os.path.join('Notebook', 'user_features.csv')\n",
    "answers_path = os.path.join('Notebook', 'answers_r42.csv')\n",
    "results_dir = os.path.join('Notebook', 'results')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Try several candidate locations for the features and answers files so the notebook runs\n",
    "# whether executed from the repo root or from inside the Notebook folder.\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "feature_candidates = [\n",
    "    features_path,\n",
    "    os.path.join(cwd, 'Notebook', 'user_features.csv'),\n",
    "    os.path.join(cwd, 'user_features.csv'),\n",
    "    os.path.join(cwd, '..', 'Notebook', 'user_features.csv'),\n",
    "    os.path.join(cwd, '..', 'user_features.csv')\n",
    "]\n",
    "answer_candidates = [\n",
    "    answers_path,\n",
    "    os.path.join(cwd, 'Notebook', 'answers_r42.csv'),\n",
    "    os.path.join(cwd, 'answers_r42.csv'),\n",
    "    os.path.join(cwd, '..', 'Notebook', 'answers_r42.csv'),\n",
    "    os.path.join(cwd, '..', 'answers_r42.csv')\n",
    "]\n",
    "\n",
    "found_feature = None\n",
    "for p in feature_candidates:\n",
    "    if os.path.exists(p):\n",
    "        features_path = p\n",
    "        found_feature = p\n",
    "        break\n",
    "\n",
    "found_answer = None\n",
    "for p in answer_candidates:\n",
    "    if os.path.exists(p):\n",
    "        answers_path = p\n",
    "        found_answer = p\n",
    "        break\n",
    "\n",
    "if not found_feature or not found_answer:\n",
    "    msg = [f\"CWD: {cwd}\", \"Checked paths for user_features.csv:\"]\n",
    "    msg.extend(feature_candidates)\n",
    "    msg.append(\"Checked paths for answers_r42.csv:\")\n",
    "    msg.extend(answer_candidates)\n",
    "    raise FileNotFoundError(\"Could not locate required input files.\\n\" + \"\\n\".join(map(str, msg)))\n",
    "\n",
    "# Load data\n",
    "features_df = pd.read_csv(features_path)\n",
    "answers_df = pd.read_csv(answers_path)\n",
    "\n",
    "print(f'Loaded features from: {features_path}')\n",
    "print(f'Loaded answers from: {answers_path}')\n",
    "\n",
    "# Normalise user keys\n",
    "if 'user' in features_df.columns and 'user' in answers_df.columns:\n",
    "    features_df['user'] = features_df['user'].astype(str).str.strip().str.upper()\n",
    "    answers_df['user'] = answers_df['user'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Merge or derive ground truth from answers_r42.csv\n",
    "if 'ground_truth' in answers_df.columns:\n",
    "    features_df = features_df.merge(answers_df[['user','ground_truth']], on='user', how='left')\n",
    "elif 'label' in answers_df.columns:\n",
    "    # Accept either 'label' or 'ground_truth' column name in answers file\n",
    "    features_df = features_df.merge(answers_df[['user','label']], on='user', how='left')\n",
    "    features_df['ground_truth'] = features_df['label'].fillna(0).astype(int)\n",
    "else:\n",
    "    # If answers file is just a list of users, mark present users as 1\n",
    "    features_df['ground_truth'] = features_df['user'].isin(answers_df['user']).astype(int)\n",
    "\n",
    "features_df['ground_truth'] = features_df['ground_truth'].fillna(0).astype(int)\n",
    "print('Ground-truth distribution (answers_r42):')\n",
    "print(features_df['ground_truth'].value_counts())\n",
    "\n",
    "# Split: train 70% / val 20% / test 10% (stratify by ground_truth)\n",
    "train_df, temp_df = train_test_split(features_df, test_size=0.3, random_state=42, stratify=features_df['ground_truth'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=1/3, random_state=42, stratify=temp_df['ground_truth'])\n",
    "print('Shapes: train, val, test =', train_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "# Helper to build numeric feature matrix (drop ids & label columns)\n",
    "drop_cols = [\"user\", \"employee_name\", \"user_id\", \"anomaly_label\", \"anomaly_score\", \"label\", \"ground_truth\"]\n",
    "def build_X(df):\n",
    "    X = df.drop(columns=drop_cols, errors='ignore')\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    return X\n",
    "\n",
    "X_train = build_X(train_df)\n",
    "X_val = build_X(val_df)\n",
    "X_test = build_X(test_df)\n",
    "\n",
    "y_train = train_df['ground_truth'].astype(int)\n",
    "y_val = val_df['ground_truth'].astype(int)\n",
    "y_test = test_df['ground_truth'].astype(int)\n",
    "\n",
    "print('Numeric feature counts: train={}, val={}, test={}'.format(X_train.shape[1], X_val.shape[1], X_test.shape[1]))\n",
    "\n",
    "# Fit scaler on train only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- New: Train a lightweight autoencoder on train set (dense AE)\n",
    "try:\n",
    "    from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    encoding_dim = max(8, min(64, input_dim // 2))\n",
    "    ae = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(encoding_dim, activation='relu'),\n",
    "        layers.Dense(max(encoding_dim//2, 4), activation='relu'),\n",
    "        layers.Dense(encoding_dim, activation='relu'),\n",
    "        layers.Dense(input_dim, activation='linear'),\n",
    "    ])\n",
    "    ae.compile(optimizer=optimizers.Adam(1e-3), loss='mse')\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = ae.fit(X_train_scaled, X_train_scaled,\n",
    "                     validation_data=(X_val_scaled, X_val_scaled),\n",
    "                     epochs=200, batch_size=32, verbose=1, callbacks=[es])\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    ae.save('models/autoencoder.keras')\n",
    "    print('Autoencoder trained and saved to models/autoencoder.keras')\n",
    "    # compute AE MSE on val and test\n",
    "    recon_val = ae.predict(X_val_scaled)\n",
    "    ae_mse_val = np.mean(np.square(X_val_scaled - recon_val), axis=1)\n",
    "    recon_test = ae.predict(X_test_scaled)\n",
    "    ae_mse_test = np.mean(np.square(X_test_scaled - recon_test), axis=1)\n",
    "    val_df['ae_mse'] = ae_mse_val\n",
    "    test_df['ae_mse'] = ae_mse_test\n",
    "except Exception as e:\n",
    "    print('Autoencoder training skipped or failed:', e)\n",
    "    ae = None\n",
    "\n",
    "# Train IsolationForest on train set only\n",
    "iso = IsolationForest(n_estimators=1000, contamination=0.05, random_state=42)\n",
    "iso.fit(X_train_scaled)\n",
    "\n",
    "# Compute scores on val and test\n",
    "scores_val = iso.decision_function(X_val_scaled)\n",
    "scores_test = iso.decision_function(X_test_scaled)\n",
    "\n",
    "# Sweep contamination percentiles on validation to pick best F1\n",
    "percs = np.linspace(0.001, 0.5, 500)\n",
    "rows = []\n",
    "for p in percs:\n",
    "    thresh = float(np.nanpercentile(scores_val[~np.isnan(scores_val)], 100 * p))\n",
    "    y_val_pred = (scores_val <= thresh).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val, y_val_pred, average='binary', zero_division=0)\n",
    "    rows.append({'contamination': float(p), 'threshold': thresh, 'precision': float(prec), 'recall': float(rec), 'f1': float(f1), 'predicted_positives': int(np.nansum(y_val_pred))})\n",
    "\n",
    "df_tune = pd.DataFrame(rows)\n",
    "best_row = df_tune.loc[df_tune['f1'].idxmax()]\n",
    "selected_contamination = float(best_row['contamination'])\n",
    "selected_threshold = float(best_row['threshold'])\n",
    "print('\\nBest validation F1:')\n",
    "print(best_row.to_dict())\n",
    "\n",
    "# Save tuning table and selected threshold\n",
    "df_tune.to_csv(os.path.join(results_dir, 'threshold_tuning.csv'), index=False)\n",
    "with open(os.path.join(results_dir, 'selected_threshold.json'), 'w') as fh:\n",
    "    json.dump({'contamination': selected_contamination, 'threshold': selected_threshold, 'f1': float(best_row['f1'])}, fh)\n",
    "print('Saved tuning table and selected threshold to', results_dir)\n",
    "\n",
    "# Evaluate on test set with selected threshold\n",
    "y_test_pred = (scores_test <= selected_threshold).astype(int)\n",
    "print('\\nTest set results (selected threshold applied):')\n",
    "print('Contamination chosen from val: {:.4f}, threshold: {:.6f}'.format(selected_contamination, selected_threshold))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_test_pred))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_test_pred, target_names=['Normal','Insider'], zero_division=0))\n",
    "\n",
    "# Save per-user anomaly scores/labels for the whole dataset (use scaler fitted on train + iso fitted on train)\n",
    "# Ensure the full feature set uses the same columns and ordering as the training features\n",
    "full_X = build_X(features_df)\n",
    "try:\n",
    "    # If X_train is a DataFrame, reindex full_X to match training columns (fill missing with 0)\n",
    "    if hasattr(X_train, 'columns'):\n",
    "        feature_cols = X_train.columns.tolist()\n",
    "        full_X = full_X.reindex(columns=feature_cols, fill_value=0)\n",
    "except Exception:\n",
    "    # Fall back to using full_X as-is if anything unexpected occurs\n",
    "    pass\n",
    "\n",
    "# Now safe to transform with the scaler fitted on the training set\n",
    "full_X_scaled = scaler.transform(full_X)\n",
    "full_scores = iso.decision_function(full_X_scaled)\n",
    "full_df_out = features_df[['user']].copy()\n",
    "full_df_out['anomaly_score'] = full_scores\n",
    "# If autoencoder was trained, compute AE MSE on the full set and attach\n",
    "if 'ae' in globals() and ae is not None:\n",
    "    try:\n",
    "        recon_full = ae.predict(full_X_scaled)\n",
    "        full_df_out['ae_mse'] = np.mean(np.square(full_X_scaled - recon_full), axis=1)\n",
    "    except Exception as e:\n",
    "        print('Failed to compute AE mse on full set:', e)\n",
    "\n",
    "full_df_out['anomaly_label'] = (full_df_out['anomaly_score'] <= selected_threshold).astype(int)\n",
    "full_df_out.to_csv(os.path.join(results_dir, 'anomaly_scores_by_user.csv'), index=False)\n",
    "print('Saved per-user anomaly scores/labels to', os.path.join(results_dir, 'anomaly_scores_by_user.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fcd6f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth counts:\n",
      " ground_truth\n",
      "0    930\n",
      "1     70\n",
      "Name: count, dtype: int64\n",
      "Predicted counts:\n",
      " anomaly_label\n",
      "0    950\n",
      "1     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confusion Matrix:\n",
      " [[892  38]\n",
      " [ 58  12]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.94      0.96      0.95       930\n",
      "     Insider       0.24      0.17      0.20        70\n",
      "\n",
      "    accuracy                           0.90      1000\n",
      "   macro avg       0.59      0.57      0.57      1000\n",
      "weighted avg       0.89      0.90      0.90      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "\n",
    "# Actual labels (from insider truth file)\n",
    "y_true = features_df[\"ground_truth\"].astype(int)\n",
    "\n",
    "# Load saved predictions (results/anomaly_scores_by_user.csv) and align by user\n",
    "res_path = os.path.join(\"results\", \"anomaly_scores_by_user.csv\")\n",
    "if not os.path.exists(res_path):\n",
    "    raise FileNotFoundError(f\"Results file not found at {res_path}\")\n",
    "res = pd.read_csv(res_path)\n",
    "\n",
    "# Standardize user column if present\n",
    "if 'user' in features_df.columns and 'user' in res.columns:\n",
    "    left = features_df[['user']].copy()\n",
    "    left['user'] = left['user'].astype(str).str.strip().str.upper()\n",
    "    res['user'] = res['user'].astype(str).str.strip().str.upper()\n",
    "    merged = left.merge(res, on='user', how='left')\n",
    "else:\n",
    "    # No user alignment possible — try to use order-preserving predictions\n",
    "    merged = res.copy()\n",
    "\n",
    "# Decide predicted labels: prefer anomaly_label column, else infer from anomaly_score using contamination\n",
    "if 'anomaly_label' in merged.columns:\n",
    "    y_pred = merged['anomaly_label'].fillna(0).astype(int)\n",
    "elif 'anomaly_score' in merged.columns:\n",
    "    # Anomaly score from IsolationForest: lower -> more anomalous\n",
    "    scores = merged['anomaly_score'].astype(float)\n",
    "    # default contamination fallback\n",
    "    contamination = 0.05\n",
    "    thresh = np.nanpercentile(scores.dropna(), 100 * contamination)\n",
    "    # predict anomaly if score <= thresh (anomalies are low scores)\n",
    "    y_pred = (scores <= thresh).astype(int).fillna(0)\n",
    "else:\n",
    "    raise KeyError('No anomaly_label or anomaly_score column found in results file.')\n",
    "\n",
    "# If merged includes extra rows or alignment used, ensure lengths match by selecting same index as features_df\n",
    "if 'user' in features_df.columns and 'user' in merged.columns:\n",
    "    # merged length equals features_df\n",
    "    pass\n",
    "else:\n",
    "    # If shapes mismatch, attempt to trim/pad\n",
    "    if len(y_pred) != len(y_true):\n",
    "        raise ValueError(f\"Length mismatch after loading predictions: y_true={len(y_true)}, y_pred={len(y_pred)}. Ensure results file aligns with features_df or contains 'user' column for merging.\")\n",
    "\n",
    "# Print class distribution and diagnostics\n",
    "print(\"Ground-truth counts:\\n\", y_true.value_counts())\n",
    "print(\"Predicted counts:\\n\", pd.Series(y_pred).value_counts())\n",
    "\n",
    "# Generate metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Normal\", \"Insider\"], zero_division=0)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a180e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
